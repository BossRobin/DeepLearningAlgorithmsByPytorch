{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "mount_file_id": "1wvgFOQWgoUBBUrM-FQEk5vjGVz7JMHUP",
      "authorship_tag": "ABX9TyM7xDxyyGcYtkFodkn7CGjp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BossRobin/DeepLearningAlgorithmsByPytorch/blob/master/NLG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngBjKmSQ-cR2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.nn.utils import clip_grad_norm_ # 梯度裁剪\n",
        "\n",
        "class Dictionary(object):\n",
        "  def __init__(self):\n",
        "    self.word2idx = {}\n",
        "    self.idx2word = {}\n",
        "    self.idx = 0\n",
        "  \n",
        "  def add_word(self, word):\n",
        "    if not word in self.word2idx:\n",
        "      self.word2idx[word] = self.idx\n",
        "      self.idx2word[self.idx] = word\n",
        "      self.idx += 1\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.word2idx)\n",
        "\n",
        "\n",
        "class Corpus(object): # 语料库类\n",
        "  def __init__(self):\n",
        "    self.dictionary = Dictionary() # 语料库对应的字典\n",
        "\n",
        "  def get_data(self, path, batch_size=20):\n",
        "    # Add words to the dictionary\n",
        "    with open(path, 'r') as f:\n",
        "      tokens = 0\n",
        "      for line in f:\n",
        "        words = line.split() + ['<eos>']\n",
        "        tokens += len(words)\n",
        "        for word in words: \n",
        "          self.dictionary.add_word(word)  \n",
        "\n",
        "    # Tokenize the file content\n",
        "    ids = torch.LongTensor(tokens) # LongTensor是pytorch中的一种类型， tokens大小为语料库的大小加上结束符的个数\n",
        "    token = 0\n",
        "    with open(path, 'r') as f:\n",
        "      for line in f:\n",
        "        words = line.split() + ['<eos>']\n",
        "        for word in words:\n",
        "          ids[token] = self.dictionary.word2idx[word] # 遍历语料库，将所有单词序列化\n",
        "          token += 1\n",
        "\n",
        "    num_batches = ids.size(0) // batch_size # 因为ids即tokens的大小可能无法整除batch_size，所以将最后一部分截掉\n",
        "    ids = ids[:num_batches*batch_size]\n",
        "    return ids.view(batch_size, -1) # 返回语料库序列化后按照batch_size大小分块的数据"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "un7btZ8qBD3e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlUcYCvRBQ6v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embed_size = 128 # embedding层的节点数，即词嵌入层的维度\n",
        "hidden_size = 1024\n",
        "num_layers = 1\n",
        "num_epochs = 5\n",
        "num_samples = 1000 # 最终生成词的数量\n",
        "batch_size = 20 # 一个batch的大小\n",
        "seq_length = 30 # 每个batch中的一个seq的长度\n",
        "learning_rate = 0.002"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SygMcqOBtCO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e4bf538c-e28e-482a-f306-fbedf62abb61"
      },
      "source": [
        "corpus = Corpus()\n",
        "ids = corpus.get_data('data/train.txt', batch_size)\n",
        "vocab_size = len(corpus.dictionary) # 字典大小\n",
        "num_batches = ids.size(1) # batch数"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOjjTkG3CGXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNLM(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
        "    super(RNNLM, self).__init__()\n",
        "    self.embed = nn.Embedding(vocab_size, embed_size) # embedding层\n",
        "    self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True) # lstm层\n",
        "    self.linear = nn.Linear(hidden_size, vocab_size) # 全连接层\n",
        "  \n",
        "  def forward(self, x, h):\n",
        "    x = self.embed(x)\n",
        "    out, (h, c) = self.lstm(x, h) # out的size是torch.Size([20, 30, 1024]),20是batch_size，30是seq_length，1024是隐藏层的维度\n",
        "    out = out.reshape(out.size(0)*out.size(1), out.size(2)) # 需要将out进行转换，以输入全连接层。转换方法是将一个batch的序列合成一串。\n",
        "\n",
        "    out = self.linear(out)\n",
        "    return out, (h, c)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_Pg7gHdCScB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = RNNLM(vocab_size, embed_size, hidden_size, num_layers).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F308s-NFDrBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "def detach(states):\n",
        "  return [state.detach() for state in states] # detach()就是截断反向传播的梯度流"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zo8ZfGMD-4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "  states = (torch.zeros(num_layers, batch_size, hidden_size).to(device),\n",
        "        torch.zeros(num_layers, batch_size, hidden_size).to(device)) # states 相当于h0,c0\n",
        "\n",
        "  for i in range(0, ids.size(1) - seq_length, seq_length): # ids的size为torch.Size([20, 46479])。将输入流分割成[20, 30]大小的数据作为一个批次的inputs\n",
        "    inputs = ids[:, i:i+seq_length].to(device)\n",
        "    targets = ids[:, (i+1):(i+1)+seq_length].to(device)  # targets是每个单词的下一个单词，因此此程序的目的是完成生成任务\n",
        "\n",
        "    states = detach(states)  # 将h和c的梯度归0？\n",
        "    outputs, states = model(inputs, states)\n",
        "    loss = criterion(outputs, targets.reshape(-1))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    clip_grad_norm_(model.parameters(), 0.5)  # 进行梯度截断，防止梯度爆炸或者梯度消失\n",
        "    optimizer.step()\n",
        "\n",
        "    step = (i+1) // seq_length\n",
        "    if step % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
        "                   .format(epoch+1, num_epochs, step, num_batches // seq_length, loss.item(), np.exp(loss.item())))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eIZgx-mFw1P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with torch.no_grad():\n",
        "  with open('sample.txt', 'w') as f:\n",
        "    state = (torch.zeros(num_layers, 1, hidden_size).to(device),\n",
        "          torch.zeros(num_layers, 1, hidden_size).to(device))\n",
        "    # 随机选择一个词作为输入\n",
        "    prob = torch.ones(vocab_size)\n",
        "    input = torch.multinomial(prob, num_samples=1).unsqueeze(1).to(device)\n",
        "    # torch.multinomial(input, num_samples,replacement=False, out=None) → LongTensor\n",
        "    # 作用是对input的每一行做n_samples次取值，输出的张量是每一次取值时input张量对应行的下标。\n",
        "    # input张量可以看成一个权重张量，每一个元素代表其在该行中的权重。如果有元素为0，那么在其他不为0的元素\n",
        "    # 被取干净之前，这个元素是不会被取到的。\n",
        "    # n_samples是每一行的取值次数，该值不能大于每一样的元素数，否则会报错。\n",
        "    # replacement指的是取样时是否是有放回的取样，True是有放回，False无放回。\n",
        "\n",
        "    for i in range(num_samples):\n",
        "      output, state = model(input, state)\n",
        "      prob = output.exp() # 将output的值取e的对数，即转化为概率\n",
        "      word_id = torch.multinomial(prob, num_samples=1).item()\n",
        "\n",
        "      input.fill_(word_id) # 将input中的内容替换为新的词\n",
        "      word = corpus.dictionary.idx2word[word_id]\n",
        "      word = '\\n' if word == '<eos>' else word + ' '\n",
        "      f.write(word)\n",
        "\n",
        "      if (i+1) % 100 == 0:\n",
        "        print('Sample [{}/{}] words and save to {}'.format(i+1, num_samples, 'sample.txt'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hh7YsL6tJLc_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}